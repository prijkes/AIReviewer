# ============================================================================
# Quaally Configuration File
# ============================================================================
#
# This file contains default settings for Quaally behavior.
#
# IMPORTANT:
#   - This file contains STATIC settings (baseline defaults)
#   - DYNAMIC settings (credentials, PR info) MUST be set via environment variables
#   - To override static settings, use the --env command-line option to load a .env file
#
# Configuration Loading:
#   1. settings.ini - Read first (static baseline defaults)
#   2. Environment variables - Only for required dynamic values (credentials, paths)
#   3. .env file (optional) - Loaded via --env flag to override static settings
#
# After changing settings:
#   - No recompilation needed
#   - Just restart the application
#
# To use different settings:
#   - For static settings: Use --env flag to load a .env file
#   - For dynamic settings: Set environment variables directly
#
# See README.md for setup instructions.
# See PIPELINE.md for required environment variables.
#
# ============================================================================


# ============================================================================
# [AI Model Configuration]
# Settings that control the Azure OpenAI model behavior
# ============================================================================

[AI]

# ----------------------------------------------------------------------------
# Deployment
# ----------------------------------------------------------------------------
# The name of your Azure OpenAI deployment (not the base model name).
# Find this in Azure OpenAI Studio under "Deployments".
#
# Common Options:
#   - gpt-4           = Highest quality, most expensive, slower
#   - gpt-4-turbo     = Good balance of quality/speed/cost
#   - o4-mini         = Fastest, cheapest, good for simple reviews
#
# Cost Impact: High
# When to Change: Based on budget vs quality requirements
#
# Default: o4-mini
Deployment = o4-mini

# ----------------------------------------------------------------------------
# Temperature
# ----------------------------------------------------------------------------
# Controls randomness/creativity in AI responses (0.0 to 1.0)
#
# How it works:
#   - 0.0 = Completely deterministic (same input = same output)
#   - 0.2 = Slight variation, mostly consistent (RECOMMENDED for reviews)
#   - 0.5 = Balanced creativity and consistency
#   - 1.0 = Maximum creativity and variation
#
# For code reviews, use LOW values (0.1-0.3) to ensure:
#   - Consistent feedback
#   - Focused on actual issues
#   - Less "creative" interpretations
#
# Cost Impact: None
# When to Change: Only if reviews are too repetitive (increase) or too random (decrease)
#
# Default: 0.2
Temperature = 0.2

# ----------------------------------------------------------------------------
# MaxTokens
# ----------------------------------------------------------------------------
# Maximum number of tokens the AI can generate in its response
#
# Understanding tokens:
#   - Roughly 1 token = 0.75 words
#   - 2000 tokens ≈ 1500 words
#
# Guidelines by value:
#   - 1000  = Brief reviews, simple changes only
#   - 2000  = Standard reviews (RECOMMENDED)
#   - 4000  = Complex reviews with detailed explanations
#   - 8000  = Very detailed reviews (significantly higher cost)
#
# Cost Impact: High (directly affects API charges)
# When to Change:
#   - Increase: If reviews are truncated or lack detail
#   - Decrease: To reduce costs for simple PRs
#
# Default: 2000
MaxTokens = 2000


# ============================================================================
# [Function Calling Configuration]
# Controls whether AI can request additional context during reviews
# ============================================================================

[FunctionCalling]

# ----------------------------------------------------------------------------
# Enabled
# ----------------------------------------------------------------------------
# Enable OpenAI function calling feature
#
# When ENABLED, the AI can call functions to:
#   - Get full file contents (not just the diff)
#   - Search the entire codebase for patterns
#   - View file commit history
#   - Find related files
#
# Trade-offs:
#   
#   PROS:
#     ✓ More informed, context-aware reviews
#     ✓ Can detect breaking changes across files
#     ✓ Better understanding of code architecture
#     ✓ Can verify assumptions
#
#   CONS:
#     ✗ 2-5x higher API costs per review
#     ✗ Slower reviews (5-15 seconds vs 2-5 seconds)
#     ✗ More complex to debug
#     ✗ Higher token usage
#
# Recommendations:
#   - false = Quick reviews, simple changes, cost-sensitive deployments
#   - true  = Complex PRs, large codebases, breaking changes, quality > cost
#
# Cost Impact: Very High (multiplies API calls)
# When to Change: Based on PR complexity and budget
#
# Default: false
Enabled = false

# ----------------------------------------------------------------------------
# MaxCalls
# ----------------------------------------------------------------------------
# Maximum number of function calls the AI can make per file review
#
# Purpose:
#   - Prevents infinite loops
#   - Controls API costs
#   - Limits review time
#
# Guidelines:
#   - 1-3   = Minimal context retrieval, lower cost
#   - 5     = Good balance (RECOMMENDED)
#   - 10+   = Allow deep investigation, higher cost, longer reviews
#
# Note: Only applies when Enabled = true
#
# Cost Impact: Medium (when function calling is enabled)
# When to Change:
#   - Increase: If AI needs more context to understand changes
#   - Decrease: To reduce costs and review time
#
# Default: 5
MaxCalls = 5

# ----------------------------------------------------------------------------
# MaxConversationIterations
# ----------------------------------------------------------------------------
# Maximum number of conversation iterations in a single AI interaction.
#
# Purpose:
#   - Prevents infinite loops in function calling scenarios
#   - The AI can call multiple functions in sequence during a conversation
#   - Each function call counts as one iteration
#   - Once this limit is reached, the conversation ends
#
# Guidelines:
#   - 5     = Quick conversations, limited context gathering
#   - 10    = Standard depth (RECOMMENDED)
#   - 20+   = Deep analysis, complex problem solving
#
# Note: Only applies when Enabled = true
#
# Cost Impact: High (more iterations = more AI API calls)
# When to Change:
#   - Increase: For complex troubleshooting requiring multiple function calls
#   - Decrease: To reduce costs and limit conversation depth
#
# Default: 10
MaxConversationIterations = 10

# ----------------------------------------------------------------------------
# MaxDiffSizeBytes
# ----------------------------------------------------------------------------
# Maximum size in bytes for diff content returned by get_pr_diff function.
#
# Purpose:
#   - When AI calls get_pr_diff function, large diffs are truncated
#   - Prevents exceeding token limits
#   - Keeps function responses focused
#
# Guidelines:
#   - 4000 (4KB)  = Brief diffs only
#   - 8000 (8KB)  = Standard diff size (RECOMMENDED)
#   - 16000 (16KB) = Allow larger diffs
#
# Note: Only applies when function calling is enabled and AI uses get_pr_diff
#
# Cost Impact: Medium (larger diffs = more tokens in function responses)
# When to Change:
#   - Increase: If diff truncation loses important context
#   - Decrease: To reduce token usage
#
# Default: 8000
MaxDiffSizeBytes = 8000


# ============================================================================
# [Review Behavior Settings]
# Control how the review process operates
# ============================================================================

[Review]

# ----------------------------------------------------------------------------
# DryRun
# ----------------------------------------------------------------------------
# Test mode - performs review without posting to Azure DevOps
#
# When true:
#   - Reviews are performed normally
#   - Issues are identified
#   - Results are logged to console
#   - NO comments posted to Azure DevOps
#   - NO approvals/votes set
#
# Use cases:
#   - Testing configuration changes
#   - Previewing reviews locally
#   - CI/CD pipeline testing
#   - Debugging review logic
#
# Cost Impact: None
# When to Change: Set to true for testing, false for production
#
# Default: false
DryRun = false

# ----------------------------------------------------------------------------
# OnlyReviewIfRequiredReviewer
# ----------------------------------------------------------------------------
# Only run the review if the PAT user has been added as a required reviewer
#
# When true:
#   - Before performing any review, checks if the PAT user is a required reviewer
#   - If NOT a required reviewer, skips the review entirely
#   - Logs a message and exits gracefully
#   - No comments posted, no approval set
#
# When false:
#   - Reviews all PRs regardless of reviewer requirements
#   - Standard behavior (current default)
#
# Use cases:
#   - Enforce that the bot is explicitly requested via required reviewer
#   - Prevent automatic reviews on all PRs (opt-in model)
#   - Control review costs by limiting to specific PRs
#   - Allow teams to manually opt-in for AI review
#
# Cost Impact: None (only controls when reviews run)
# When to Change:
#   - Set to true: To require explicit opt-in via required reviewer
#   - Set to false: To review all PRs automatically
#
# Default: false
OnlyReviewIfRequiredReviewer = false

# ----------------------------------------------------------------------------
# Scope
# ----------------------------------------------------------------------------
# Defines which files to review in the pull request
#
# Current options:
#   - changed-files = Only review files modified in the PR (RECOMMENDED)
#
# Future options (not yet implemented):
#   - all-files = Review all files in repository
#   - custom    = Custom file selection logic
#
# Cost Impact: None (only one option currently)
# When to Change: Currently no alternatives
#
# Default: changed-files
Scope = changed-files

# ----------------------------------------------------------------------------
# WarnBudget
# ----------------------------------------------------------------------------
# Maximum number of warnings before rejecting approval
#
# How it works:
#   - Errors ALWAYS reject approval (regardless of this setting)
#   - Warnings only reject if count exceeds this budget
#   - Example: If WarnBudget=3 and PR has 4 warnings, approval is rejected
#
# Approval logic:
#   - Approve: No errors AND warnings ≤ WarnBudget
#   - Reject:  Has errors OR warnings > WarnBudget
#
# Guidelines:
#   - 0     = Reject on ANY warning (very strict)
#   - 3     = Allow minor issues (RECOMMENDED)
#   - 10+   = Very lenient (warnings rarely block)
#
# Cost Impact: None
# When to Change: Based on your team's quality standards
#
# Default: 3
WarnBudget = 3

# ----------------------------------------------------------------------------
# PolicyPath
# ----------------------------------------------------------------------------
# Path to the markdown file containing review policy/guidelines
#
# The AI uses this policy to guide its review. Include:
#   - Coding standards
#   - Security requirements
#   - Performance guidelines
#   - Best practices
#   - Team conventions
#
# Format: Relative path from application directory
#
# Example policy sections:
#   ## Security
#   - Validate all user inputs
#   - Use parameterized queries
#   
#   ## Performance
#   - Avoid N+1 queries
#   - Cache expensive operations
#
# Cost Impact: Minimal (affects prompt size)
# When to Change: Update path if you reorganize files
#
# Default: ./policy/review-policy.md
PolicyPath = ./Resources/policy/review-policy.md

# ----------------------------------------------------------------------------
# PromptsBasePath
# ----------------------------------------------------------------------------
# Base path to the directory containing prompt template files
#
# The prompt templates guide how the AI reviewer communicates and structures
# its reviews. Templates are organized in subdirectories:
#   - system/       = Language-specific base prompts (C#, C++, etc.)
#   - instructions/ = Task-specific instructions (file review, metadata review)
#   - language/     = Response language directives (English, Japanese)
#
# Benefits of external prompts:
#   ✓ Edit prompts without recompiling code
#   ✓ Version control prompt changes separately
#   ✓ Easy A/B testing of different prompts
#   ✓ Non-developers can refine prompts
#   ✓ Consistent with policy file approach
#
# Format: Relative path from application directory
#
# Example structure:
#   ./prompts/
#     ├── system/
#     │   ├── base-csharp.md
#     │   ├── base-cpp.md
#     │   └── ...
#     ├── instructions/
#     │   ├── file-review.md
#     │   └── metadata-review.md
#     └── language/
#         ├── english.md
#         └── japanese.md
#
# Cost Impact: Minimal (affects prompt construction)
# When to Change: Update path if you reorganize prompt files
#
# Default: ./prompts
PromptsBasePath = ./Resources/prompts


# ============================================================================
# [File Processing Settings]
# Control which files are reviewed and how much content is processed
# ============================================================================

[Files]

# ----------------------------------------------------------------------------
# MaxFilesToReview
# ----------------------------------------------------------------------------
# Maximum number of files to review in a single pull request
#
# Why limit files?
#   - Control API costs
#   - Prevent timeouts on huge PRs
#   - Ensure timely feedback
#   - Avoid overwhelming developers
#
# What happens if exceeded?
#   - Only the first N files are reviewed
#   - Remaining files are skipped
#   - Warning is logged
#
# Guidelines:
#   - 20    = Small PRs, quick feedback, low cost
#   - 50    = Most PRs (RECOMMENDED)
#   - 100+  = Large refactorings, migrations (higher cost/time)
#
# Cost Impact: High (more files = more API calls)
# When to Change:
#   - Increase: For large, legitimate PRs
#   - Decrease: To reduce costs and review time
#
# Default: 50
MaxFilesToReview = 50

# ----------------------------------------------------------------------------
# MaxIssuesPerFile
# ----------------------------------------------------------------------------
# Maximum number of issues to report per file
#
# Why limit issues?
#   - Focus on most important problems
#   - Prevent overwhelming developers
#   - Reduce noise in PR comments
#   - Keep reviews actionable
#
# What happens if exceeded?
#   - AI identifies all issues
#   - Only top N issues (by severity) are reported
#   - Others are discarded
#
# Guidelines:
#   - 3     = Only critical issues
#   - 5     = Focused feedback (RECOMMENDED)
#   - 10+   = Comprehensive review (may be overwhelming)
#
# Cost Impact: None (AI still reviews fully)
# When to Change:
#   - Increase: For thorough reviews
#   - Decrease: To keep feedback focused
#
# Default: 5
MaxIssuesPerFile = 5

# ----------------------------------------------------------------------------
# MaxFileBytes
# ----------------------------------------------------------------------------
# Maximum file size (in bytes) to review
# Larger files are skipped entirely
#
# Why limit file size?
#   - Large files are expensive to review
#   - Often auto-generated or vendor code
#   - May exceed AI context window
#   - Usually not worth reviewing
#
# Common file sizes:
#   - 100KB = Small file
#   - 200KB = Medium file (RECOMMENDED limit)
#   - 500KB = Large file
#   - 1MB   = Very large file
#
# What happens if exceeded?
#   - File is skipped completely
#   - No review performed
#   - Warning logged
#
# Format: Supports human-readable sizes (200KB, 1.5MB, 1GB) or plain numbers (bytes)
#
# Cost Impact: Medium (larger files = more tokens = higher cost)
# When to Change:
#   - Increase: To review larger files
#   - Decrease: To skip more large files (reduce cost)
#
# Default: 200KB
MaxFileBytes = 200KB

# ----------------------------------------------------------------------------
# MaxDiffBytes
# ----------------------------------------------------------------------------
# Maximum diff size (in bytes) to send to AI
# Larger diffs are truncated before processing
#
# Why limit diff size?
#   - Prevent exceeding AI context window
#   - Control API costs (fewer tokens)
#   - Large diffs often contain noise
#   - Focus on meaningful changes
#
# What happens if exceeded?
#   - Diff is truncated to this size
#   - Warning logged
#   - Review proceeds with truncated diff
#
# Guidelines:
#   - 200KB = Conservative limit
#   - 500KB = Standard limit (RECOMMENDED)
#   - 1MB   = Allow large diffs (higher cost)
#
# Format: Supports human-readable sizes (200KB, 1.5MB, 1GB) or plain numbers (bytes)
#
# Cost Impact: High (diff size directly affects tokens used)
# When to Change:
#   - Increase: For large but legitimate changes
#   - Decrease: To reduce token usage and costs
#
# Default: 500KB
MaxDiffBytes = 500KB

# ----------------------------------------------------------------------------
# MaxPromptDiffBytes
# ----------------------------------------------------------------------------
# Maximum diff size (in bytes) to include in the AI prompt
# This is a secondary truncation that happens during prompt construction
#
# Difference from MaxDiffBytes:
#   - MaxDiffBytes      = Hard limit before retrieval
#   - MaxPromptDiffBytes = Soft limit in prompt (this setting)
#
# Why a separate limit?
#   - Keep prompts focused
#   - Stay well under token limits
#   - Improve review quality (less noise)
#
# What happens if exceeded?
#   - Diff is truncated in the prompt
#   - AI sees less context
#   - Review may be less comprehensive
#
# Guidelines:
#   - 4KB  = Brief context
#   - 8KB  = Standard context (RECOMMENDED)
#   - 16KB = Detailed context (higher cost)
#
# Format: Supports human-readable sizes (8KB, 1.5MB) or plain numbers (bytes)
#
# Cost Impact: High (directly affects input tokens)
# When to Change:
#   - Increase: If AI needs more context
#   - Decrease: To reduce token usage
#
# Default: 8KB
MaxPromptDiffBytes = 8KB

# ----------------------------------------------------------------------------
# MaxCommitMessagesToReview
# ----------------------------------------------------------------------------
# Maximum number of commit messages to include in PR metadata review
#
# What is metadata review?
#   - Reviews PR title, description, and commit messages
#   - Checks for completeness and clarity
#   - Ensures good PR hygiene
#
# Why limit commit messages?
#   - Old commits may not be relevant
#   - Too many messages = noise
#   - Keep metadata review focused
#
# What happens if exceeded?
#   - Only the most recent N commit messages are reviewed
#   - Older commits are ignored
#
# Guidelines:
#   - 5     = Recent commits only
#   - 10    = Good overview (RECOMMENDED)
#   - 20+   = Full history (may include irrelevant commits)
#
# Cost Impact: Low (minimal tokens)
# When to Change:
#   - Increase: For PRs with meaningful commit history
#   - Decrease: To focus on most recent changes only
#
# Default: 10
MaxCommitMessagesToReview = 10


# ============================================================================
# [Language Detection Settings]
# Control how the AI detects the language for reviews
# ============================================================================

[Language]

# ----------------------------------------------------------------------------
# JapaneseDetectionThreshold
# ----------------------------------------------------------------------------
# Threshold ratio (0.0 to 1.0) for detecting Japanese language in PR descriptions
#
# How it works:
#   1. Count Japanese characters (Hiragana, Katakana, Kanji, Fullwidth)
#   2. Count total non-whitespace characters
#   3. Calculate ratio = japanese_chars / total_chars
#   4. If ratio > threshold, use Japanese for reviews
#   5. Otherwise, use English
#
# Examples with threshold = 0.3:
#   - "Fix bug in login"
#     → 0% Japanese → English
#
#   - "ログイン機能のバグ修正"
#     → 100% Japanese → Japanese
#
#   - "Fix ログイン bug"
#     → ~20% Japanese (< 30%) → English
#
# Guidelines:
#   - 0.1 (10%)  = Very sensitive (detect minimal Japanese)
#   - 0.3 (30%)  = Balanced detection (RECOMMENDED)
#   - 0.5 (50%)  = Only clearly Japanese text
#
# Affects:
#   - Code review comments
#   - PR metadata feedback
#   - All issue titles, rationales, recommendations
#
# Cost Impact: None
# When to Change: If language detection is too sensitive or not sensitive enough
#
# Default: 0.3 (30%)
JapaneseDetectionThreshold = 0.3


# ============================================================================
# [Provider Configuration]
# Select which source control provider to use
# ============================================================================

[Provider]

# ----------------------------------------------------------------------------
# SourceProvider
# ----------------------------------------------------------------------------
# The source control provider to use for pull request reviews
#
# Supported providers:
#   - AzureDevOps = Azure DevOps (default)
#   - GitHub      = GitHub (coming soon)
#   - GitLab      = GitLab (coming soon)
#   - Bitbucket   = Bitbucket (planned)
#
# Provider-specific settings:
#   Each provider has its own configuration file in the Providers folder:
#   - Azure DevOps: Providers/AzureDevOps/azure.ini
#   - GitHub:       Providers/GitHub/github.ini (future)
#   - GitLab:       Providers/GitLab/gitlab.ini (future)
#
# Provider-specific environment variables:
#   Each provider has its own .env file:
#   - Azure DevOps: Providers/AzureDevOps/azure.env
#   - GitHub:       Providers/GitHub/github.env (future)
#   - GitLab:       Providers/GitLab/gitlab.env (future)
#
# Cost Impact: None
# When to Change: When switching to a different source control provider
#
# Default: AzureDevOps
SourceProvider = AzureDevOps


# ============================================================================
# End of Generic Configuration
# ============================================================================
#
# Remember:
#   - This file contains GENERIC settings for all providers
#   - Provider-specific settings are in Providers/{ProviderName}/*.ini
#   - Dynamic values (credentials, etc.) are in environment variables
#   - See README.md for setup instructions
#
# ============================================================================
